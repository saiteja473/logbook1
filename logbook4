import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Load the dataset
data_stocks = pd.read_csv('data_stocks.csv')

# Prepare the data: Assuming the target is the last column
X = data_stocks.iloc[:, :-1].values  # Features
y = data_stocks.iloc[:, -1].values   # Target

# Scale the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define the MLP model
mlp_model = tf.keras.Sequential([
    tf.keras.layers.Dense(107, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(53, activation='relu'),
    tf.keras.layers.Dense(1)  # Output layer for regression
])

# Compile the model
mlp_model.compile(optimizer='adam', loss='mae', metrics=['mae'])

# Train the model
history = mlp_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)

# Evaluate the model on the test set
test_loss, test_mae = mlp_model.evaluate(X_test, y_test, verbose=1)

# Print the model summary
mlp_model.summary()

# Print the Mean Absolute Error
print("\nMean Absolute Error (MAE) on the test set:", test_mae)
Epoch 1/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 6.2114 - mae: 6.2114 - val_loss: 0.5646 - val_mae: 0.5646
Epoch 2/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.6349 - mae: 0.6349 - val_loss: 0.3497 - val_mae: 0.3497
Epoch 3/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - loss: 0.4576 - mae: 0.4576 - val_loss: 0.2904 - val_mae: 0.2904
Epoch 4/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - loss: 0.4026 - mae: 0.4026 - val_loss: 0.5818 - val_mae: 0.5818
Epoch 5/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - loss: 0.3672 - mae: 0.3672 - val_loss: 0.2354 - val_mae: 0.2354
Epoch 6/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.3655 - mae: 0.3655 - val_loss: 0.2326 - val_mae: 0.2326
Epoch 7/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 7s 7ms/step - loss: 0.3519 - mae: 0.3519 - val_loss: 0.2074 - val_mae: 0.2074
Epoch 8/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 0.3104 - mae: 0.3104 - val_loss: 0.4902 - val_mae: 0.4902
Epoch 9/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - loss: 0.3319 - mae: 0.3319 - val_loss: 0.1987 - val_mae: 0.1987
Epoch 10/10
1032/1032 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.3213 - mae: 0.3213 - val_loss: 0.2837 - val_mae: 0.2837
258/258 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.2830 - mae: 0.2830
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense (Dense)                   │ (None, 107)            │        53,714 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 53)             │         5,724 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (Dense)                 │ (None, 1)              │            54 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 178,478 (697.18 KB)
 Trainable params: 59,492 (232.39 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 118,986 (464.79 KB)

Mean Absolute Error (MAE) on the test set: 0.28372257947921753
